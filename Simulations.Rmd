---
title: "MB4 Data Simulations and Analysis"
author: "Kelsey Lucca, Arthur Capelier-Mourguy, & Mike Frank"
date: "10/29/2019"
abstract: "This document provides evidence from simulated data for choices made in the ManyBabies4 study. Using data constructed with respects to the MB4 study structure and with effect sizes as predicted by a recent meta-analysis, we show that (a) models converge better when using scaled and centred, instead of raw, age in days, and (b) models converge better with informative priors, but those priors are not strong enough to overcome a true null effect in the data. For a detail of the empricial study and the context surrounding it, see the MB4 manuscript."
output:
  pdf_document: default
  html_document: default
---

```{r library imports, echo=1:10, message=FALSE, warning=FALSE}
library(brms)
library(bridgesampling)
library(tidyverse)
library(scales)
library(beepr)
library(future)
library(future.apply)
library(furrr)
plan(multiprocess, workers = 4) # Adapt to the number of cores you want to use
source("StatTools.R")
source("geom_flat_violin.R")

theme_set(theme_bw())
knitr::opts_chunk$set(cache = TRUE)
```

# Introduction

First, we generate data, either with the expected effects (score above chance, no effect of age), or with a true null effect (no preference). The size of the dataset is taken from previous ManyBabies studies. Precisely, we set the number of participants tested in each lab from the mean and standard deviation observed in MB1, with a minimum of 16 participants as required by MB4, and downsize the number of labs from 67 to 20 to reduce computation times and draw conclusions that will still be robust with less participating labs than could be expected.

For the "true effect"" data sets, we use the estimated true effect size from a recent meta-analysis (Margoni & Surian, 2018) for the mean probability of chosing the helper versus hinderer character, and the 95% Confidence Interval and average sample size in studies included in Margoni and Surian (2018) to compute the standard deviation.
For the "null effect" data sets, we use the same standard deviation but an equal probability of chosing either character.

```{r data generation, message=FALSE, warning=FALSE}
# Define simulation-specific variables
n_labs <- 20
infants_by_lab <- rnorm(n_labs, 35, 20) %>% round() %>% pmax.int(16)
n_infants <- sum(infants_by_lab)
age_min <- 165
age_max <- 320
pr_helper <- 0.64     # From Margoni & Surian 2018 (estimated true effect size)
pr_helper_sd <- 0.1   # From Margoni & Surian 2018 (CI*sqrt(mean(N))/Z(95))

# Generate repeat datasets
n_sims <- 30 # The code shown in the rest of this document is restricted to one simulation
data_sims <- n_sims %>%
  future_replicate(tibble(lab_id = mapply(rep,
                                          paste0("lab",1:n_labs),
                                          infants_by_lab) %>%
                            unlist() %>%
                            as_factor(),
                          age_days = sample(age_min:age_max, n_infants,
                                            replace = T),
                          z_age_days = scale(age_days),
                          chose_helper = rbinom(n_infants, 1,
                                                rnorm(n_infants,
                                                      mean = pr_helper,
                                                      sd = pr_helper_sd))),
                   simplify = F)
null_sims <- n_sims %>%
  future_replicate(tibble(lab_id = mapply(rep,
                                          paste0("lab",1:n_labs),
                                          infants_by_lab) %>%
                            unlist() %>%
                            as_factor(),
                          age_days = sample(age_min:age_max, n_infants,
                                            replace = T),
                          z_age_days = scale(age_days),
                          chose_helper = rbinom(n_infants, 1,
                                                rnorm(n_infants,
                                                      mean = .5,
                                                      sd = pr_helper_sd))),
                   simplify = F)
```

We define informative priors based on Margoni and Surian (2018) for the Intercept, mildly informative priors on the main effect of age (expected null), and priors more restrictive than the defaults on the standard deviation of random effects to help the model converge. Crucially, sampling is of insufficient quality without those priors, which leads to errors when bridge-sampling the posterior to compute Bayes factors. We will see however that those priors do not lead to ''significant'' findings when there the data are constructed with a true null effect.

```{r bayesian priors}
priors.full <- c(set_prior("normal(.5753641, .1)", # From Margoni & Surian, through logit
                           class = "Intercept"),
                 set_prior("normal(0, .5)",
                           class = "b"),
                 set_prior("student_t(3, 0, 2)",
                           class = "sd"))
priors.nointercept <- c(set_prior("normal(0, .5)",
                                  class = "b"),
                        set_prior("student_t(3, 0, 2)",
                                  class = "sd"))
priors.noage <- c(set_prior("normal(.5753641, .1)", # From Margoni & Surian, through logit
                           class = "Intercept"),
                 set_prior("student_t(3, 0, 2)",
                           class = "sd"))
```

# Age (in days): raw *versus* scaled and centred

We first test the differences between running Bayesian general linear models on raw age in days (`age_days`) and scaled and centred age in days (`z_age_days`), precisely in terms of model convergence, precision of parameter estimations, and detection of an effect with a model comparison and a Bayes factor.

## Using raw `age_days`

### Models

We fit a full model and a no-intercept model to the true effect data, use bridge sampling to estimate posterior distributions, and finally compare the two models to obtain a Bayes factor in favour of the full model over the no-intercept model. First, we get the full model and bridge sample it's posterior:

```{r eval=FALSE}
model.raw_age.full <- brm(chose_helper ~ age_days + (age_days | lab_id),
                          data = data_sim, family = bernoulli,
                          prior = priors.full, iter = 10000,
                          future = T, save_all_pars = T,
                          control = list(adapt_delta = .9999,
                                         max_treedepth = 20))
bridge.raw_age.full <- bridge_sampler(model.raw_age.full, silent = T)
```

We then run the same code for the no-intercept model, that is, using the following formula:

```{r eval=FALSE}
chose_helper ~ 0 + age_days + (0 + age_days | lab_id)
```

Finally, we compute a Bayes factor from the sampled posteriors.

```{r eval=FALSE}
bf.raw_age <- bayes_factor(bridge.raw_age.full,
                           bridge.raw_age.nointercept)
```

```{r raw age, echo=FALSE, message=FALSE, warning=FALSE}
save_path <- "simulation_results/raw_age/"
# Run bayesian models, bridge-sample, then
# save parameter estimates and HPDIs to csv
# Running the models takes several hours
run_models <- F
if(run_models){
  ## Initialise estimates column names if file doesn't exist
  if(!file.exists(paste0(save_path, "estimates_full.csv"))){
    estimates <- tibble(`Parameter` = character(),
                        `Estimate` = double(),
                        `Est. Error` = double(),
                        `lower` = double(),
                        `upper` = double(),
                        `95% CI` = character(),
                        .rows = 0)
    write_csv(estimates, paste0(save_path, "estimates_full.csv"))
  }
  out <- lapply(1:n_sims, function(i){
    tryCatch(
      {
        ## Full model
        print(paste("Starting raw models", i))
        ### Run model
        m <- brm(chose_helper ~ age_days +
                   (age_days | lab_id),
                 data = data_sims[[i]],
                 family = bernoulli,
                 prior = priors.full,
                 iter = 10000,
                 future = T,
                 save_all_pars = T,
                 control = list(adapt_delta = .9999,
                                max_treedepth = 20))
        ### Get and save estimates and HPDIs
        estimates <- estimates.brm_fixef(m, prob = .95, digits = 7)
        write_csv(estimates, paste0(save_path, "estimates_full.csv"), append = T)
        ### Bridge-sample posterior
        bridge.full <- bridge_sampler(m, silent = T)
        ## No Intercept
        ### Run model
        m <- brm(chose_helper ~ 0 + age_days +
                   (0 + age_days | lab_id),
                 data = data_sims[[i]],
                 family = bernoulli,
                 prior = priors.nointercept,
                 iter = 10000,
                 future = T,
                 save_all_pars = T,
                 control = list(adapt_delta = .9999,
                                max_treedepth = 20))
        ### Bridge-sample posterior
        bridge.nointercept <- bridge_sampler(m, silent = T)
        ## Compute and save Bayes factor
        bf <- bayes_factor(bridge.full,
                           bridge.nointercept)$bf
        write_csv(enframe(bf, name = NULL), # To write the single number as a tibble
                  paste0(save_path, "bayes_factors.csv"), append = T)
        return(0)
        },
      error = function(cond){return(1)})
  })
  beep(8)
}
## Read Bayes factors
bf.raw_age <- read_csv(paste0(save_path, "bayes_factors.csv"),
                       col_names = "bf")
```

### Plots

We can look at the parameter estimates and their credible intervals for all the models, compared to the true simulated effects:

```{r raw age estimates, message=FALSE, warning=FALSE}
# Read estimates and HPDIs
estimates.raw_age.full <- read_csv(paste0(save_path, "estimates_full.csv")) %>%
  rownames_to_column() %>%
  as_tibble()
# True simulated effects
true_effects <- tibble(Parameter = c("Intercept", "age_days"),
                       Value = c(.5753641, 0))
# Plot
ggplot(estimates.raw_age.full,
       aes(x = rowname, y = Estimate,
           ymin = lower, ymax = upper,
           colour = Parameter)) +
  xlab("Simulation") + ylab("Parameter Estimate") + coord_flip() +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  scale_colour_brewer(palette = "Dark2", guide = FALSE) +
  facet_grid(cols = vars(Parameter), scales = "free_x") +
  geom_hline(data = true_effects, aes(yintercept = Value), linetype = "dashed") +
  geom_pointrange()
```

We see that the estimate for age_days is pretty good, but the estimate for the intercept is much noisier. Notably, the estimate for `age_days` and it's HPDI are very close to zero as this parameter relates to raw age in days, which has values ranging from 165 to 320.

## Using scaled and centred `z_age_days`

### Models

We fit a full model and a no-intercept model to the true effect data, use bridge sampling to estimate posterior distributions, and finally compare the two models to obtain a Bayes factor in favour of the full model over the no-intercept model. The code used is the same, with the difference that `age_days` is replaced by `z_age_days`.

```{r scaled centred age, echo=FALSE, message=FALSE, warning=FALSE}
save_path <- "simulation_results/scaled_age/"
# Run bayesian models, bridge-sample, then
# save parameter estimates and HPDIs to csv
# Running the models takes several hours
run_models <- F
if(run_models){
  ## Initialise estimates column names if file doesn't exist
  if(!file.exists(paste0(save_path, "estimates_full.csv"))){
    estimates <- tibble(`Parameter` = character(),
                        `Estimate` = double(),
                        `Est. Error` = double(),
                        `lower` = double(),
                        `upper` = double(),
                        `95% CI` = character(),
                        .rows = 0)
    write_csv(estimates, paste0(save_path, "estimates_full.csv"))
  }
  out <- lapply(1:n_sims, function(i){
    tryCatch(
      {
        ## Full model
        print(paste("Starting scaled models", i))
        ### Run model
        m <- brm(chose_helper ~ z_age_days +
                   (z_age_days | lab_id),
                 data = data_sims[[i]],
                 family = bernoulli,
                 prior = priors.full,
                 iter = 10000,
                 future = T,
                 save_all_pars = T,
                 control = list(adapt_delta = .9999,
                                max_treedepth = 20))
        ### Get and save estimates and HPDIs
        estimates <- estimates.brm_fixef(m, prob = .95, digits = 7)
        write_csv(estimates, paste0(save_path, "estimates_full.csv"), append = T)
        ### Bridge-sample posterior
        bridge.full <- bridge_sampler(m, silent = T)
        ## No Intercept
        ### Run model
        m <- brm(chose_helper ~ 0 + z_age_days +
                   (0 + z_age_days | lab_id),
                 data = data_sims[[i]],
                 family = bernoulli,
                 prior = priors.nointercept,
                 iter = 10000,
                 future = T,
                 save_all_pars = T,
                 control = list(adapt_delta = .9999,
                                max_treedepth = 20))
        ### Bridge-sample posterior
        bridge.nointercept <- bridge_sampler(m, silent = T)
        ## Compute and save Bayes factor for Intercept
        bf <- bayes_factor(bridge.full,
                           bridge.nointercept)$bf
        write_csv(enframe(bf, name = NULL), # To write the single number as a tibble
                  paste0(save_path, "bayes_factors_intercept.csv"), append = T)
        ## No Age (necessary here while we have access to full model bridge)
        ### Run model
        m <- brm(chose_helper ~ 1 + (1 | lab_id),
                 data = data_sims[[i]],
                 family = bernoulli,
                 prior = priors.noage,
                 iter = 10000,
                 future = T,
                 save_all_pars = T,
                 control = list(adapt_delta = .9999,
                                max_treedepth = 20))
        ### Bridge-sample posterior
        bridge.noage <- bridge_sampler(m, silent = T)
        ## Compute and save Bayes factor for Age
        bf <- bayes_factor(bridge.full,
                           bridge.noage)$bf
        write_csv(enframe(bf, name = NULL), # To write the single number as a tibble
                  paste0(save_path, "bayes_factors_age.csv"), append = T)
        return(0)
        },
      error = function(cond){return(1)})
  })
  beep(8)
}
## Read Bayes factors
bf.scaled_age <- read_csv(paste0(save_path, "bayes_factors_intercept.csv"),
                          col_names = "bf")
bf.noage <- read_csv(paste0(save_path, "bayes_factors_age.csv"),
                     col_names = "bf")
```

### Plots

We can look at the parameter estimates and their credible intervals for all the models, compared to the true simulated effects:

```{r scaled age estimates, echo=FALSE, message=FALSE, warning=FALSE}
# Read estimates and HPDIs
estimates.scaled_age.full <- read_csv(paste0(save_path,"estimates_full.csv")) %>%
  rownames_to_column() %>%
  as_tibble()
# True simulated effects
true_effects <- tibble(Parameter = c("Intercept", "z_age_days"),
                       Value = c(.5753641, 0))
# Plot
ggplot(estimates.scaled_age.full,
       aes(x = rowname, y = Estimate,
           ymin = lower, ymax = upper,
           colour = Parameter)) +
  xlab("Simulation") + ylab("Parameter Estimate") + coord_flip() +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  scale_colour_brewer(palette = "Dark2", guide = FALSE) +
  facet_grid(cols = vars(Parameter), scales = "free_x") +
  geom_hline(data = true_effects, aes(yintercept = Value), linetype = "dashed") +
  geom_pointrange()
```

This time we see that parameter estimates for the intercept are much closer to the true effect and much more precise. Notably, this time the estimate for `z_age_days` is still centred on zero but much noisier, as this parameter now relates to one standard deviation in the sample for age in days, on average 40 days.

## Comparing `age_days` and `z_age_days`

As we have seen, the parameter estimates were evidently closer to the true effects when using scaled and centred age compared to using raw age. A crucial measure however is the models' ability to detect the true effect, as measured in our scripts by a model comparison Bayes factor (*b*-values). We can plot those *b*-values for models using `age_days` and `z_age_days`, with dashed line representing the typically used values for evidence for the null (*b*=1/3) and for the alternative hypothesis (*b*=3):

```{r raw scaled bayes factors, message=FALSE, warning=FALSE}
# Get Bayes factors into a proper tibble
bf.raw_age.tmp <- bf.raw_age %>%
  mutate(age = "raw age")
bf.scaled_age.tmp <- bf.scaled_age %>%
  mutate(age = "scaled centred age")
bf.all <- bind_rows(bf.raw_age.tmp, bf.scaled_age.tmp) %>%
  mutate(log_bf = log1p(bf))
# Raincloud plot
ggplot(bf.all, aes(x = age, y = log_bf,
                   colour = age, fill = age)) +
  theme_bw() + ylab("Log Bayes factor [log(b+1)]") +
  coord_flip() + facet_grid(cols = vars(age), scales = "free_x") +
  theme(legend.position = "top",
        axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  geom_hline(yintercept = c(log1p(1/3), log1p(3)), linetype = "dashed") +
  geom_flat_violin(position = position_nudge(x = .2),
                   colour = "black", alpha = .5, width = .7) +
  geom_point(position = position_jitter(width = .15),
             size = 1, alpha = .6, show.legend = FALSE) +
  geom_boxplot(width = .1, alpha = .3, outlier.shape = NA,
               colour = "black", show.legend = FALSE) +
  scale_color_brewer(palette = "Dark2", name = NULL) +
  scale_fill_brewer(palette = "Dark2", name = NULL)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
evidence <- bf.all %>%
  group_by(age) %>%
  summarise(n_H1 = sum(bf >= 3),
            n_H0 = sum(bf <= 1/3))
n_H1.raw_age <- evidence %>% subset(age == "raw age", n_H1) %>% as.numeric()
n_H0.raw_age <- evidence %>% subset(age == "raw age", n_H0) %>% as.numeric()
```

Clearly, we see that models using raw `age_days` do not detect the true simulated effect, with only `r n_H1.raw_age` models resulting in a $b\geq3$, and sometimes even give evidence for the null with a Bayes factor $b\leq {}^1\!/_3$ (*n*=`r n_H0.raw_age`). On the other hand, models using scaled and centred `z_age_days` reliably provided robust evidence for the true simulated effect.

Thus, we use `z_age_days` in the MB4 analysis script.

# Evidence for a true null effect for age

Another important question in the MB4 study is whether or not there is an effect of age on probability of chosing the helper vs. hinderer character. Here, we check how reliably the Bayesian analysis proposed (using scaled and centred `z_age_days`) provides evidence for a true null effect of age.

## Models

We first run a model that includes only the intercept but no effect of age (`chose_helper ~ 1 + (1 | lab_id)`), and compare this model with the full model to obtain a Bayes factor as before.

## Plots

We can then plot those *b*-values, with dashed lines representing the typical decision values for evidence for the null ($b\leq {}^1\!/_3$) and against the null ($b\geq3$):

```{r no age bayes factors, echo=FALSE, message=FALSE, warning=FALSE}
# Get Bayes factors into a proper tibble
bf.noage <- bf.noage %>%
  mutate(test = "no age",
         log_bf = log1p(bf))
# Raincloud plot
ggplot(bf.noage, aes(x = test, y = log_bf,
                   colour = test, fill = test)) +
  theme_bw() + ylab("Log Bayes factor [log(b+1)]") + coord_flip() +
  theme(legend.position = "top",
        axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  geom_hline(yintercept = c(log1p(1/3), log1p(3)), linetype = "dashed") +
  geom_flat_violin(position = position_nudge(x = .2),
                   colour = "black", alpha = .5, width = .7) +
  geom_point(position = position_jitter(width = .15),
             size = 1, alpha = .6, show.legend = FALSE) +
  geom_boxplot(width = .1, alpha = .3, outlier.shape = NA,
               colour = "black", show.legend = FALSE) +
  scale_color_brewer(palette = "Dark2", name = NULL) +
  scale_fill_brewer(palette = "Dark2", name = NULL)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
evidence <- bf.noage %>%
  summarise(n_H1 = sum(bf >= 3),
            n_H0 = sum(bf <= 1/3))
n_H1.noage <- evidence %>% select(n_H1) %>% as.numeric()
n_H0.noage <- evidence %>% select(n_H0) %>% as.numeric()
```

Here, we see that the *b*-values consistently indicate that a true null effect is better supported by the data.

# Impact of informative priors on true null data

We used informative priors in the previous simulations, to help models converge, and to adopt a more Bayesian approach. Here, we wanted to verify that those informative priors would not mislead models to find evidence for a non-null effect when there was no such effect in the data.

## Models

We first run the same statistical models, using scaled and centred `z_age_days`, on data with a true null effect, that is, no preference to either helper or hinderer character, and compare a full model to a "no intercept" model to obtain *b*-values.

```{r true null, echo=FALSE, message=FALSE, warning=FALSE}
save_path <- "simulation_results/true_null/"
# Run bayesian models, bridge-sample, then
# save parameter estimates and HPDIs to csv
# Running the models takes several hours
run_models <- F
if(run_models){
  ## Initialise estimates column names if file doesn't exist
  if(!file.exists(paste0(save_path, "estimates_full.csv"))){
    estimates <- tibble(`Parameter` = character(),
                        `Estimate` = double(),
                        `Est. Error` = double(),
                        `lower` = double(),
                        `upper` = double(),
                        `95% CI` = character(),
                        .rows = 0)
    write_csv(estimates, paste0(save_path, "estimates_full.csv"))
  }
  out <- lapply(1:n_sims, function(i){
    tryCatch(
      {
        ## Full model
        print(paste("Starting true null models", i))
        ### Run model
        m <- brm(chose_helper ~ z_age_days +
                   (z_age_days | lab_id),
                 data = null_sims[[i]],
                 family = bernoulli,
                 prior = priors.full,
                 iter = 10000,
                 future = T,
                 save_all_pars = T,
                 control = list(adapt_delta = .9999,
                                max_treedepth = 20))
        ### Get and save estimates and HPDIs
        estimates <- estimates.brm_fixef(m, prob = .95, digits = 7)
        write_csv(estimates, paste0(save_path, "estimates_full.csv"), append = T)
        ### Bridge-sample posterior
        bridge.full <- bridge_sampler(m, silent = T)
        ## No Intercept
        ### Run model
        m <- brm(chose_helper ~ 0 + z_age_days +
                   (0 + z_age_days | lab_id),
                 data = null_sims[[i]],
                 family = bernoulli,
                 prior = priors.nointercept,
                 iter = 10000,
                 future = T,
                 save_all_pars = T,
                 control = list(adapt_delta = .9999,
                                max_treedepth = 20))
        ### Bridge-sample posterior
        bridge.nointercept <- bridge_sampler(m, silent = T)
        ## Compute and save Bayes factor
        bf <- bayes_factor(bridge.full,
                           bridge.nointercept)$bf
        write_csv(enframe(bf, name = NULL), # To write the single number as a tibble
                  paste0(save_path, "bayes_factors.csv"), append = T)
        return(0)
        },
      error = function(cond){return(1)})
  })
  beep(8)
}
## Read Bayes factors
bf.true_null <- read_csv(paste0(save_path, "bayes_factors.csv"),
                         col_names = "bf")
```

## Plots

As a first sanity check, we can plot the parameter estimates for the full model, with informative priors, on null data:

```{r true null estimates, echo=FALSE, message=FALSE, warning=FALSE}
# Get estimates and HPDIs
estimates.true_null.full <- read_csv(paste0(save_path, "estimates_full.csv")) %>%
  rownames_to_column() %>%
  as_tibble()
# True simulated effects
true_effects <- tibble(Parameter = c("Intercept", "z_age_days"),
                       Value = c(0, 0))
# Plot
ggplot(estimates.true_null.full,
       aes(x = rowname, y = Estimate,
           ymin = lower, ymax = upper,
           colour = Parameter)) +
  xlab("Simulation") + ylab("Parameter Estimate") + coord_flip() +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  scale_colour_brewer(palette = "Dark2", guide = FALSE) +
  facet_grid(cols = vars(Parameter), scales = "free_x") +
  geom_hline(data = true_effects, aes(yintercept = Value), linetype = "dashed") +
  geom_pointrange()
```

First, we see that overall the informative priors have indeed shifted the estimated Intercept away from zero. More precisely however, the addition of true null data has shifted our initial beliefs (informative priors) closer to a zero intercept, and additional data would further change our knowledge through Bayesian update.

We can then plot the *b*-values associated with those models:

```{r true null bayes factors, echo=FALSE, message=FALSE, warning=FALSE}
# Get Bayes factors into a proper tibble
bf.true_null <- bf.true_null %>%
  mutate(test = "true null",
         log_bf = log1p(bf))
# Raincloud plot
ggplot(bf.true_null, aes(x = test, y = log_bf,
                   colour = test, fill = test)) +
  theme_bw() + ylab("Log Bayes factor [log(b+1)]") + coord_flip() +
  theme(legend.position = "top",
        axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  geom_hline(yintercept = c(log1p(1/3), log1p(3)), linetype = "dashed") +
  geom_flat_violin(position = position_nudge(x = .2),
                   colour = "black", alpha = .5, width = .7) +
  geom_point(position = position_jitter(width = .15),
             size = 1, alpha = .6, show.legend = FALSE) +
  geom_boxplot(width = .1, alpha = .3, outlier.shape = NA,
               colour = "black", show.legend = FALSE) +
  scale_color_brewer(palette = "Dark2", name = NULL) +
  scale_fill_brewer(palette = "Dark2", name = NULL)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
evidence <- bf.true_null %>%
  summarise(n_H1 = sum(bf >= 3),
            n_H0 = sum(bf <= 1/3))
n_H1.true_null <- evidence %>% select(n_H1) %>% as.numeric()
n_H0.true_null <- evidence %>% select(n_H0) %>% as.numeric()
```

We see that, despite the intercept estimate still being shifted away from zero, the Bayesian hypothesis testing consistently tells us that the null hypothesis is better supported by the data at hand. Given that we know the data to be constructed around a true null effect, this shows that the informative priors constructed from Margoni and Surian (2018) did not impact the conclusions we would draw from true null data.

# Cost, in terms of statistical power, of adding a control condition

One issue that was raised by the first round of reviews was the absence of a non-social control condition. There were theoretical reasons in favour and against the addition of this control condition, but a crucial factor to make a final decision was the impact this would have on statistical power, and therefore the impact on the number of participants required to evidence a true effect.

To answer this question, we run by hand a Bayesian power analysis, following the structure described in [this blog post](https://solomonkurz.netlify.com/post/bayesian-power-analysis-part-i/). In short, we simulate data, run two models to be compared with a Bayes factor as previously, and count which percentage of models evidenced an effect as conveyed by a $b$-value greater than 3. To cut down on computation time (from a couple months to a day), we run 100 simulations for each tested number of participants $n$, and we conduct a dychotomic search between 200 and 600 participants to find the number of participants necessary to achieve roughly 80% power.

## Simulations

For the paradigm without a non-social control condition, the data and tests are the ones described previously, using `z_age_days` to test for a non-null intercept.

For the paradigm with a non-social control condition, we expect participants in the control condition to have no preference and be as variable as participants in the social condition. We then test the following models against each other:

```{r eval=FALSE}
# Model for H1
chose_helper ~ 1 + condition + age_days + (1 + condition + age_days | lab_id)
# Model for H0
chose_helper ~ 1 +             age_days + (1 +             age_days | lab_id)
```

```{r echo=FALSE}
save_path <- "simulation_results/control/"
# Running all the models takes days, and saves Bayes factors for later use
run_models <- T
if(run_models){
  n_sims <- 8
  # Control simulations
  n_infants <- 10
  diff_helper <- pr_helper - .5
  ## Initial control dataset
  df.ctrl <- tibble(lab_id = sample(1:n_labs, n_infants, replace = T) %>% as_factor(),
                    age_days = sample(age_min:age_max, n_infants, replace = T),
                    z_age_days = scale(age_days),
                    condition = rep(c("non-social", "social"), each = n_infants/2),
                    chose_helper = rbinom(n_infants, 1,
                                          rnorm(n_infants,
                                                mean = .5 + diff_helper*(condition=="social"),
                                                sd = pr_helper_sd)))
  ## Control priors
  priors.ctrl.H1 <- c(set_prior("normal(0, .1)",
                                class = "Intercept"),
                      set_prior("normal(0, .5)",
                                class = "b"),
                      set_prior("normal(.5753641, .1)", # From Margoni & Surian, through logit
                                class = "b", coef = "conditionsocial"),
                      set_prior("student_t(3, 0, 2)",
                                class = "sd"))
  priors.ctrl.H0 <- c(set_prior("normal(0, .1)",
                                class = "Intercept"),
                      set_prior("normal(0, .5)",
                                class = "b"),
                      set_prior("student_t(3, 0, 2)",
                                class = "sd"))
  ## Control models
  model.ctrl.H1 <- brm(chose_helper ~ 1 + condition + z_age_days +
                         (1 + condition + z_age_days | lab_id),
                       data = df.ctrl,
                       family = bernoulli,
                       prior = priors.ctrl.H1,
                       iter = 10000,
                       future = T,
                       save_all_pars = T,
                       control = list(adapt_delta = .9999,
                                      max_treedepth = 20))
  model.ctrl.H0 <- brm(chose_helper ~ 1 + z_age_days +
                         (1 + z_age_days | lab_id),
                       data = df.ctrl,
                       family = bernoulli,
                       prior = priors.ctrl.H0,
                       iter = 10000,
                       future = T,
                       save_all_pars = T,
                       control = list(adapt_delta = .9999,
                                      max_treedepth = 20))
  ## Define main simulation function
  sim.ctrl <- function(seed, n_infants, alt = F){
    set.seed(seed)
    ### Generate dataset
    df <- tibble(lab_id = sample(1:n_labs, n_infants, replace = T) %>% as_factor(),
                 age_days = sample(age_min:age_max, n_infants, replace = T),
                 z_age_days = scale(age_days),
                 condition = rep(c("non-social", "social"), each = n_infants/2),
                 chose_helper = rbinom(n_infants, 1,
                                       rnorm(n_infants,
                                             mean = .5 + diff_helper*(condition=="social"),
                                             sd = pr_helper_sd)))
    ### Run models
    m.H1 <- update(model.ctrl.H1, newdata = df, seed = seed)
    m.H0 <- update(model.ctrl.H0, newdata = df, seed = seed)
    ### Bridge sample
    bridge.H1 <- bridge_sampler(m.H1, silent = T)
    bridge.H0 <- bridge_sampler(m.H0, silent = T)
    ### Bayes factor
    bf <- bayes_factor(bridge.H1, bridge.H0)$bf
    return(bf)
  }
  # No-control simulations
  ## Initial no-control dataset
  df.noctrl <- tibble(lab_id = sample(1:n_labs, n_infants, replace = T) %>% as_factor(),
                      age_days = sample(age_min:age_max, n_infants, replace = T),
                      z_age_days = scale(age_days),
                      chose_helper = rbinom(n_infants, 1,
                                            rnorm(n_infants,
                                                  mean = pr_helper,
                                                  sd = pr_helper_sd)))
  ## No-control priors
  priors.noctrl.H1 <- c(set_prior("normal(.5753641, .1)", # From Margoni & Surian, through logit
                                  class = "Intercept"),
                        set_prior("normal(0, .5)",
                                  class = "b"),
                        set_prior("student_t(3, 0, 2)",
                                  class = "sd"))
  priors.noctrl.H1.alt <- c(set_prior("normal(.5753641, .1)", # From Margoni & Surian, through logit
                                      class = "b", coef = "intercept"),
                            set_prior("normal(0, .5)",
                                      class = "b"),
                            set_prior("student_t(3, 0, 2)",
                                      class = "sd"))
  priors.noctrl.H0 <- c(set_prior("normal(0, .5)",
                                  class = "b"),
                        set_prior("student_t(3, 0, 2)",
                                  class = "sd"))
  ## No-control models
  model.noctrl.H1 <- brm(chose_helper ~ 1 + z_age_days +
                           (1 + z_age_days | lab_id),
                         data = df.noctrl,
                         family = bernoulli,
                         prior = priors.noctrl.H1,
                         iter = 10000,
                         future = T,
                         save_all_pars = T,
                         control = list(adapt_delta = .9999,
                                        max_treedepth = 20))
  model.noctrl.H1.alt <- brm(chose_helper ~ 0 + intercept + z_age_days +
                               (0 + intercept + z_age_days | lab_id),
                             data = df.noctrl,
                             family = bernoulli,
                             prior = priors.noctrl.H1.alt,
                             iter = 10000,
                             future = T,
                             save_all_pars = T,
                             control = list(adapt_delta = .9999,
                                            max_treedepth = 20))
  model.noctrl.H0 <- brm(chose_helper ~ 0 + z_age_days +
                           (0 + z_age_days | lab_id),
                         data = df.noctrl,
                         family = bernoulli,
                         prior = priors.noctrl.H0,
                         iter = 10000,
                         future = T,
                         save_all_pars = T,
                         control = list(adapt_delta = .9999,
                                        max_treedepth = 20))
  ## Define main simulation function
  sim.noctrl <- function(seed, n_infants, alt = F){
    set.seed(seed)
    ### Generate dataset
    df <- tibble(lab_id = sample(1:n_labs, n_infants, replace = T) %>% as_factor(),
                 age_days = sample(age_min:age_max, n_infants, replace = T),
                 z_age_days = scale(age_days),
                 chose_helper = rbinom(n_infants, 1,
                                       rnorm(n_infants,
                                             mean = pr_helper,
                                             sd = pr_helper_sd)))
    ### Run models
    if(alt){
      m.H1 <- update(model.noctrl.H1.alt, newdata = df, seed = seed)
    }else{
      m.H1 <- update(model.noctrl.H1, newdata = df, seed = seed)
      print("Is it fucking working?")
    }
    m.H0 <- update(model.noctrl.H0, newdata = df, seed = seed)
    ### Bridge sample
    bridge.H1 <- bridge_sampler(m.H1, silent = T)
    bridge.H0 <- bridge_sampler(m.H0, silent = T)
    ### Bayes factor
    bf <- bayes_factor(bridge.H1, bridge.H0)$bf
    return(bf)
  }
  # Run all simulations
  ## Dichotomic search function
  search.dichotomy <- function(sim.fun, sim.type, n_min, n_max, pwr_goal = .8){
    n <- (n_min + n_max) %/% 2
    repeat{
      print(paste0("Dichotomic search: n=", n))
      filename <- paste0(save_path, "bf_", sim.type, "_", n, ".csv")
      n_previous <- ifelse(file.exists(filename), length(read_lines(filename)) - 1, 0)
      bfs <- future_map_dbl(n_previous + 1:n_sims, possibly(sim.fun, NA),
                            n_infants = n*10, alt = str_detect(sim.type, "alt"),
                            .progress = T)
      bfs %>% enframe(name = NULL, value = "b.value") %>%
        write_csv(filename, append = file.exists(filename))
      pwr <- sum(bfs > 3, na.rm = T)/n_sims
      if(pwr < pwr_goal){
        n_min <- n
      }else{
        n_max <- n
      }
      if(n_max - n_min <= 1){break}
      n <- (n_min + n_max) %/% 2
    }
    return(n)
  }
  ## Start search for control and no-control
  n_infants_crit.ctrl <-search.dichotomy(sim.ctrl, "ctrl", 30, 61)
  n_infants_crit.noctrl <- search.dichotomy(sim.noctrl, "no-ctrl", 10, 31)
  #n_infants_crit.noctrl.alt <- search.dichotomy(sim.noctrl, "no-ctrl_alt", 10, 31)
  beep(8)
}
```

## Plots

We can then plot the computed power estimates (note that, because of the dychotomic search, we don't have power estimates at regular intervals):

```{r message=FALSE, warning=FALSE}
# Read Bayes factors for ctrl and noctrl, compute power
pwr.design <- tibble(design = as_factor(c("_ctrl_", "_no-ctrl_"))) %>%
  mutate(n10_infants = map(design, function(design){
    list.files(save_path, pattern = paste0(design, "[[:digit:]]")) %>%
      str_extract("[:digit:]+") %>%
      as.numeric()
  })) %>%
  unnest(n10_infants) %>%
  mutate(beta = map2_dbl(design, n10_infants, function(design, n){
    read_csv(paste0(save_path, "bf", design, n, ".csv")) %>%
      deframe() %>%
      {sum(.>3)/length(.)}
  }))
# Plot power for each design
ggplot(pwr.design,
       aes(x = n10_infants,
           y = beta,
           colour = design)) +
  theme(legend.position = "top") +
  geom_point(position = position_dodge(width = 1)) +
  geom_line(position = position_dodge(width = 1)) +
  geom_hline(yintercept = .8, linetype = "dashed") +
  scale_color_brewer(palette = "Dark2",
                     breaks = c("_ctrl_", "_no-ctrl_"),
                     labels = c("control", "no control"))
```

Overall this doesn't make much sense. First the power doesn't grow as it should, which is most likely due to the fact that we only had 100 simulations for each $n$.

Secondly, the power is much greater for the design with a control group than for the design without a control group, *for the same total number of participants*, which is clearly the most surprising result compared to what we know of frequentist power analysis. This might here be due to the differences in internal computation for the Intercept (tested in the no-control design) and for other model coefficients (such as `condition` tested in the control design). As such, these simulations seem to indicate that, for the question considered and the statistical approach taken, collecting and analysing a control group, with the same total number of participants, would greatly increase statistical power.

Finally, to our surprise, the maximum number of participants simulated (490 participants) is only nearly sufficient to reach a power of 80%.
